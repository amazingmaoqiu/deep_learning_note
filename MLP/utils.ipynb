{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import pdb\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "0\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "1\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "2\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "3\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "4\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "5\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "6\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "7\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "8\n",
      "(3, 784)\n",
      "(3, 100)\n",
      "(3, 100)\n",
      "(3, 10)\n",
      "(3, 100)\n",
      "(3, 100)\n",
      "9\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "(100, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/test/python_test/cs231n/assignment3/py3/lib/python3.5/site-packages/ipykernel_launcher.py:149: RuntimeWarning: divide by zero encountered in log\n",
      "/home/wei/test/python_test/cs231n/assignment3/py3/lib/python3.5/site-packages/ipykernel_launcher.py:149: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "def load_data(filePath):\n",
    "    X = []\n",
    "    Y = []\n",
    "    file = open(filePath,'r')\n",
    "    input = file.readlines()\n",
    "    for line in input:\n",
    "        line_list = line.split(',')\n",
    "        Y.append(line_list[-1])\n",
    "        X.append([float(i) for i in line_list[0:-1]])\n",
    "    X.pop()\n",
    "    Y.pop()\n",
    "    X = np.array(X)\n",
    "    Y = np.array([int(i) for i in Y])\n",
    "    return X, Y\n",
    "\n",
    "#PROBLEM 1    \n",
    "def update_weights_perceptron(X, Y, weights, bias, lr):\n",
    "    #INSERT YOUR CODE HERE\n",
    "    cache = {'a':[],'z':[]}\n",
    "    z = X\n",
    "    Y_hat = np.zeros([Y.shape[0],10])\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y_hat[i][Y[i]] = 1\n",
    "    cache['z'].append(z)\n",
    "    for i in range(len(weights)-1):\n",
    "        a = np.matmul(z, weights[i]) + bias[i]\n",
    "        z = 1.0 / (1.0 + np.exp(-1.0 * a))\n",
    "        cache['a'].append(a)\n",
    "        cache['z'].append(z)\n",
    "    a = np.matmul(z, weights[len(weights)-1]) + bias[len(weights)-1]\n",
    "    output = np.exp(a) / (1.0 + np.exp(a))\n",
    "    cache['a'].append(a)\n",
    "    cache['z'].append(output)\n",
    "    loss = -1.0 * (Y_hat*np.log(output) + (1.0 - Y_hat)*np.log(1.0 - output))\n",
    "    #backpropagation\n",
    "    updated_weights, updated_bias = weights, bias\n",
    "    da = output - Y_hat\n",
    "    dw = np.dot(cache['z'][-2].T, da)\n",
    "    db = np.sum(da,axis = 0)\n",
    "    updated_weights[-1] += -1.0*lr*dw\n",
    "    updated_bias[-1]    += -1.0*lr*db\n",
    "    for i in range(len(weights)-1,0,-1):\n",
    "        dz = np.dot(da,weights[i].T)\n",
    "        da = dz*(1.0 - cache['z'][i])*cache['z'][i]\n",
    "        dw = np.dot(cache['z'][i-1].T,da)\n",
    "        db = np.sum(da,axis=0)\n",
    "        updated_weights[i-1] += -1.0*lr*dw\n",
    "        updated_bias[i-1]    += -1.0*lr*db\n",
    "    return updated_weights, updated_bias\n",
    "\n",
    "#PROBLEM 2\n",
    "def update_weights_single_layer(X, Y, weights, bias, lr):\n",
    "    #INSERT YOUR CODE HERE\n",
    "    cache = {'a':[],'z':[]}\n",
    "    z = X\n",
    "    Y_hat = np.zeros([Y.shape[0],10])\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y_hat[i][Y[i]] = 1\n",
    "    cache['z'].append(z)\n",
    "    for i in range(len(weights)-1):\n",
    "        a = np.matmul(z, weights[i]) + bias[i]\n",
    "        z = 1.0 / (1.0 + np.exp(-1.0 * a))\n",
    "        cache['a'].append(a)\n",
    "        cache['z'].append(z)\n",
    "    a = np.matmul(z, weights[len(weights)-1]) + bias[len(weights)-1]\n",
    "    output = np.exp(a) / (1.0 + np.exp(a))\n",
    "    cache['a'].append(a)\n",
    "    cache['z'].append(output)\n",
    "    loss = -1.0 * (Y_hat*np.log(output) + (1.0 - Y_hat)*np.log(1.0 - output))\n",
    "    #backpropagation\n",
    "    updated_weights, updated_bias = weights, bias\n",
    "    da = output - Y_hat\n",
    "    dw = np.dot(cache['z'][-2].T, da)\n",
    "    db = np.sum(da,axis = 0)\n",
    "    updated_weights[-1] += -1.0*lr*dw\n",
    "    updated_bias[-1]    += -1.0*lr*db\n",
    "    for i in range(len(weights)-1,0,-1):\n",
    "        dz = np.dot(da,weights[i].T)\n",
    "        da = dz*(1.0 - cache['z'][i])*cache['z'][i]\n",
    "        dw = np.dot(cache['z'][i-1].T,da)\n",
    "        db = np.sum(da,axis=0)\n",
    "        updated_weights[i-1] += -1.0*lr*dw\n",
    "        updated_bias[i-1]    += -1.0*lr*db\n",
    "    return updated_weights, updated_bias\n",
    "\n",
    "#PROBLEM 3\n",
    "def update_weights_single_layer_mean(X, Y, weights, bias, lr):\n",
    "    #INSERT YOUR CODE HERE\n",
    "    \n",
    "    return updated_weights, updated_bias\n",
    "\n",
    "#PROBLEM 4\n",
    "def update_weights_double_layer(X, Y, weights, bias, lr):\n",
    "    #INSERT YOUR CODE HERE\n",
    "    cache = {'a':[],'z':[]}\n",
    "    z = X\n",
    "    Y_hat = np.zeros([Y.shape[0],10])\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y_hat[i][Y[i]] = 1\n",
    "    cache['z'].append(z)\n",
    "    for i in range(len(weights)-1):\n",
    "        a = np.matmul(z, weights[i]) + bias[i]\n",
    "        z = 1.0 / (1.0 + np.exp(-1.0 * a))\n",
    "        cache['a'].append(a)\n",
    "        cache['z'].append(z)\n",
    "    a = np.matmul(z, weights[len(weights)-1]) + bias[len(weights)-1]\n",
    "    output = np.exp(a) / (1.0 + np.exp(a))\n",
    "    cache['a'].append(a)\n",
    "    cache['z'].append(output)\n",
    "    loss = -1.0 * (Y_hat*np.log(output) + (1.0 - Y_hat)*np.log(1.0 - output))\n",
    "    #backpropagation\n",
    "    updated_weights, updated_bias = weights, bias\n",
    "    da = output - Y_hat\n",
    "    dw = np.dot(cache['z'][-2].T, da)\n",
    "    db = np.sum(da,axis = 0)\n",
    "    updated_weights[-1] += -1.0*lr*dw\n",
    "    updated_bias[-1]    += -1.0*lr*db\n",
    "    for i in range(len(weights)-1,0,-1):\n",
    "        dz = np.dot(da,weights[i].T)\n",
    "        da = dz*(1.0 - cache['z'][i])*cache['z'][i]\n",
    "        dw = np.dot(cache['z'][i-1].T,da)\n",
    "        db = np.sum(da,axis=0)\n",
    "        updated_weights[i-1] += -1.0*lr*dw\n",
    "        updated_bias[i-1]    += -1.0*lr*db\n",
    "    return updated_weights, updated_bias\n",
    "\n",
    "#PROBLEM 5\n",
    "def update_weights_double_layer_batch(X, Y, weights, bias, lr, batch_size):\n",
    "    #INSERT YOUR CODE HERE\n",
    "    cache = {'a':[],'z':[]}\n",
    "    num_batch = Y.shape[0] // batch_size\n",
    "    Y_hat = np.zeros([Y.shape[0],10])\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y_hat[i][Y[i]] = 1\n",
    "    for i_batch in range(num_batch):\n",
    "        cache['a'].clear()\n",
    "        cache['z'].clear()\n",
    "        z = X[i_batch*batch_size:(i_batch+1)*batch_size,:]\n",
    "        cache['z'].append(z)\n",
    "        for i in range(len(weights)-1):\n",
    "            a = np.matmul(z, weights[i]) + bias[i]\n",
    "            z = 1.0 / (1.0 + np.exp(-1.0 * a))\n",
    "            cache['a'].append(a)\n",
    "            cache['z'].append(z)\n",
    "        a = np.matmul(z, weights[len(weights)-1]) + bias[len(weights)-1]\n",
    "        output = np.exp(a) / (1.0 + np.exp(a))\n",
    "        cache['a'].append(a)\n",
    "        cache['z'].append(output)\n",
    "        loss = -1.0 * (Y_hat[i_batch*batch_size:(i_batch+1)*batch_size,:]*np.log(output) + (1.0 - Y_hat[i_batch*batch_size:(i_batch+1)*batch_size,:])*np.log(1.0 - output))\n",
    "        #backpropagation\n",
    "        da = output - Y_hat[i_batch*batch_size:(i_batch+1)*batch_size,:]\n",
    "        dw = np.dot(cache['z'][-2].T, da)\n",
    "        db = np.sum(da,axis = 0)\n",
    "        weights[-1] += -1.0*lr*dw\n",
    "        bias[-1]    += -1.0*lr*db\n",
    "        for i in range(len(weights)-1,0,-1):\n",
    "            dz = np.dot(da,weights[i].T)\n",
    "            print(cache['z'][i].shape)\n",
    "            print(dz.shape)\n",
    "            da = dz*(1.0 - cache['z'][i])*cache['z'][i]\n",
    "            dw = np.dot(cache['z'][i-1].T,da)\n",
    "            db = np.sum(da,axis=0)\n",
    "            weights[i-1] += -1.0*lr*dw\n",
    "            bias[i-1]    += -1.0*lr*db\n",
    "        print(i_batch)\n",
    "            \n",
    "#     last batch\n",
    "    if(num_batch*batch_size < Y.shape[0]):\n",
    "        cache['a'].clear()\n",
    "        cache['z'].clear()\n",
    "        z = X[num_batch*batch_size:,:]\n",
    "        print(z.shape)\n",
    "        cache['z'].append(z)\n",
    "        for i in range(len(weights)-1):\n",
    "            a = np.matmul(z, weights[i]) + bias[i]\n",
    "            z = 1.0 / (1.0 + np.exp(-1.0 * a))\n",
    "            cache['a'].append(a)\n",
    "            cache['z'].append(z)\n",
    "        a = np.matmul(z, weights[len(weights)-1]) + bias[len(weights)-1]\n",
    "        print(z.shape)\n",
    "        output = np.exp(a) / (1.0 + np.exp(a))\n",
    "        cache['a'].append(a)\n",
    "        cache['z'].append(output)\n",
    "        loss = -1.0 * (Y_hat[num_batch*batch_size:,:]*np.log(output) + (1.0 - Y_hat[num_batch*batch_size:,:])*np.log(1.0 - output))\n",
    "        #backpropagation\n",
    "        da = output - Y_hat[num_batch*batch_size:,:]\n",
    "        dw = np.dot(cache['z'][-2].T, da)\n",
    "        db = np.sum(da,axis = 0)\n",
    "        weights[-1] += -1.0*lr*dw\n",
    "        bias[-1]    += -1.0*lr*db\n",
    "        for i in range(len(weights)-1,0,-1):\n",
    "            dz = np.dot(da,weights[i].T)\n",
    "            print(cache['z'][i].shape)\n",
    "            print(da.shape)\n",
    "            da = dz*(1.0 - cache['z'][i])*cache['z'][i]\n",
    "            dw = np.dot(cache['z'][i-1].T,da)\n",
    "            db = np.sum(da,axis=0)\n",
    "            weights[i-1] += -1.0*lr*dw\n",
    "            bias[i-1]    += -1.0*lr*db\n",
    "        print(num_batch)\n",
    "    updated_weights, updated_bias = weights, bias\n",
    "    print(Y_hat)\n",
    "    return updated_weights, updated_bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "0\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "1\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "2\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "3\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "4\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "5\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "6\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "7\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "(333, 100)\n",
      "8\n",
      "(3, 784)\n",
      "(3, 100)\n",
      "(3, 100)\n",
      "(3, 10)\n",
      "(3, 100)\n",
      "(3, 100)\n",
      "9\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "[[  9.17165852e-02  -1.46594795e-01  -3.50589874e-01   7.22709352e-02\n",
      "    6.38985605e-02  -2.61418915e-01  -1.69873150e-01  -4.91461358e-01\n",
      "   -4.91461424e-01   3.29800548e-01]\n",
      " [  5.27213901e-02   2.80119690e-01  -3.55136917e-01  -9.31555492e-02\n",
      "    2.86647750e-01  -6.57147107e-02  -5.44370326e-01  -7.14800884e-02\n",
      "   -1.64278845e-01   3.33300731e-01]\n",
      " [  1.65140769e-01  -1.95620404e-01  -1.83052102e-01   1.71908562e-01\n",
      "   -2.00250512e-01  -1.95767308e-01  -5.36797014e-03   1.91252866e-01\n",
      "   -3.53015131e-01   5.77959758e-01]\n",
      " [ -1.05428125e-01   1.33372379e-01  -6.09990145e-01  -3.12492281e-01\n",
      "   -6.42362663e-01  -5.79391322e-01  -6.85069823e-01  -4.70429089e-01\n",
      "   -5.91884234e-01   9.53570265e-01]\n",
      " [  1.39145466e-01  -2.69357948e-01  -9.31587227e-02  -6.35338772e-01\n",
      "    2.00476939e-01  -4.99254273e-02  -5.02541684e-01  -1.24535841e-01\n",
      "    2.33027091e-01   4.07860626e-01]\n",
      " [ -7.38533371e-02  -2.21679154e-01   2.58641269e-01  -2.33719639e-01\n",
      "    5.76472798e-02  -2.77791132e-01  -5.74798328e-01  -6.37001719e-01\n",
      "    2.23462231e-01   5.20683389e-02]\n",
      " [  2.31630267e-01  -2.18077397e-01  -6.12237545e-02  -1.63923719e-01\n",
      "   -5.15722344e-01  -2.07584527e-01   2.61038897e-02  -1.24584479e-01\n",
      "    2.34701078e-01   9.16744162e-01]\n",
      " [ -5.16799114e-02  -4.93844613e-01  -2.62267135e-01  -4.47961188e-03\n",
      "    2.69787661e-01   2.83569424e-01  -3.84512483e-01   2.15904953e-01\n",
      "   -5.06137064e-01   9.45349277e-01]\n",
      " [ -5.12053273e-01  -7.33225704e-02  -6.65509921e-01  -6.77088321e-01\n",
      "   -6.05436360e-02   1.65990087e-01  -1.97006839e-01  -2.27878006e-02\n",
      "   -2.70609598e-01   5.69841644e-01]\n",
      " [  2.42685337e-01  -4.38716873e-01  -1.52340490e-02  -3.64154689e-01\n",
      "   -4.69485109e-01  -5.33622315e-01   5.78275396e-03   2.41683171e-01\n",
      "    1.37382535e-01   3.57350303e-01]\n",
      " [ -6.22657326e-01   2.76682651e-01  -4.52781962e-01  -2.06016912e-02\n",
      "   -1.97538299e-01  -3.99480658e-01   1.17427668e-01  -5.69615225e-01\n",
      "   -5.23464826e-01   2.85808500e-01]\n",
      " [ -1.37310075e-01   1.68432079e-02  -5.91719251e-01  -4.01342523e-01\n",
      "   -4.69106747e-01   4.45628575e-02   1.86797601e-01  -3.69559867e-03\n",
      "   -6.15609485e-01   8.67522401e-01]\n",
      " [ -2.80756425e-01  -6.36662831e-01  -5.97820250e-01   2.77724914e-01\n",
      "   -6.40171830e-01   2.14408929e-01  -2.10537666e-02   2.38984790e-02\n",
      "    1.94888490e-01  -4.06103617e-03]\n",
      " [ -6.89977893e-01  -5.34188051e-01   1.49496840e-01  -4.09208948e-01\n",
      "   -5.06040220e-01  -6.11201276e-01  -2.89020610e-01   2.22421189e-01\n",
      "   -3.55152717e-01   5.91801787e-01]\n",
      " [ -3.82582037e-01  -6.04358549e-01  -3.96638260e-01   2.31422590e-01\n",
      "   -4.90541302e-01  -1.89436801e-01  -1.42288882e-01   1.57823584e-02\n",
      "   -6.73452169e-01   6.51477490e-01]\n",
      " [ -2.46740260e-01  -6.78077292e-01  -3.83609493e-01  -4.13475328e-01\n",
      "   -1.45703561e-01  -2.74647633e-01  -5.71102287e-01  -4.29291927e-01\n",
      "   -2.71336552e-01   7.27284011e-01]\n",
      " [  2.69433037e-01  -2.38788789e-01  -2.14294301e-01  -6.41146717e-01\n",
      "    2.36183538e-01  -6.64974701e-01   1.75288068e-01  -2.78097239e-01\n",
      "   -6.36978896e-01   6.77160463e-01]\n",
      " [  2.94494944e-01   2.81392176e-01  -5.99583116e-01   2.80121425e-01\n",
      "    1.66296071e-01  -4.21478936e-01   5.22727366e-02  -5.43849576e-01\n",
      "   -4.91210239e-01   6.56577699e-01]\n",
      " [ -1.16716684e-01  -6.93535264e-01  -5.00859830e-01  -6.05155428e-01\n",
      "    7.07869487e-02  -2.03668669e-02   1.79754857e-01   8.76034539e-02\n",
      "   -6.69224335e-02   2.22827872e-01]\n",
      " [ -3.96691346e-01  -4.58883060e-01  -4.23335364e-01  -6.51124398e-01\n",
      "    2.14616610e-02  -6.83834344e-01  -3.06520124e-01   2.04813648e-01\n",
      "   -5.52110043e-01   6.14623645e-01]\n",
      " [ -1.75707249e-02  -6.20392717e-01  -5.00968804e-01  -6.89371183e-01\n",
      "   -1.26757546e-02  -4.14338592e-01  -8.12164975e-02  -5.69017655e-01\n",
      "   -1.56030089e-01   9.34011490e-01]\n",
      " [ -2.25685166e-01   2.62336486e-02  -5.71797518e-02  -6.61901575e-02\n",
      "   -2.11333200e-02  -4.69109176e-01  -8.83567853e-02  -1.21646794e-01\n",
      "   -2.54601279e-01   6.43163329e-01]\n",
      " [ -2.26904491e-01  -3.50264347e-01  -3.88771528e-01  -4.30883404e-01\n",
      "    2.77828520e-01   5.06240687e-02  -1.62087396e-01  -2.19633276e-01\n",
      "    1.00573957e-01   3.19566139e-01]\n",
      " [ -1.45502055e-01   1.24989384e-01  -6.02765271e-01  -2.79039520e-02\n",
      "   -5.33316493e-01  -6.60888234e-01  -2.14547079e-01   4.71156413e-02\n",
      "   -4.68259526e-01   8.19217608e-01]\n",
      " [  2.35105098e-02  -6.82010228e-01   1.89828770e-01  -5.16588220e-01\n",
      "   -3.07037078e-01  -3.54147972e-01   9.98577126e-02   1.53026656e-01\n",
      "    2.15346481e-01   6.89116234e-01]\n",
      " [ -6.85351217e-01  -3.47898520e-01  -6.36035055e-01  -9.80492168e-02\n",
      "   -6.12087225e-01  -1.91524903e-01  -4.60838474e-02  -3.85116547e-01\n",
      "   -6.69069576e-01   6.06927764e-01]\n",
      " [ -1.01829801e-02  -5.05809285e-01  -4.77307261e-01  -6.57783553e-01\n",
      "   -2.62058638e-01  -5.36126339e-01  -2.78186656e-01  -6.36793269e-01\n",
      "   -5.10127256e-04   1.26323858e-01]\n",
      " [ -3.52769339e-01   1.62640877e-01  -2.87576834e-01  -2.34954216e-01\n",
      "    2.09269381e-01  -6.10533565e-01  -2.41678678e-03   1.41063044e-01\n",
      "   -6.38586016e-01   4.60930145e-01]\n",
      " [ -5.69164172e-01   2.83499601e-01  -5.42679301e-01  -1.48986948e-01\n",
      "   -2.12496270e-01   1.94530023e-01  -4.98419682e-02  -2.87760973e-01\n",
      "   -4.01490261e-01   3.41347530e-01]\n",
      " [ -3.00930543e-01  -6.79144109e-01   5.41412189e-02  -1.38616642e-01\n",
      "   -6.08925968e-01  -2.09827667e-01  -5.36192697e-01  -2.94270029e-01\n",
      "    2.72784338e-01   6.51655361e-01]\n",
      " [ -3.52035571e-03  -4.68777452e-02   2.41826018e-01  -4.00117108e-01\n",
      "   -2.17904138e-01   1.19559082e-02  -2.20798586e-01  -2.52577219e-01\n",
      "   -3.48158808e-01   2.18767300e-01]\n",
      " [ -1.78101234e-02  -6.20696762e-01   1.86865840e-01  -2.01848102e-01\n",
      "   -5.53658575e-01  -8.43150362e-02  -3.09648082e-01  -4.11784194e-01\n",
      "    1.77864983e-01   6.49041498e-01]\n",
      " [  2.65400064e-01  -1.51213130e-02  -5.22098429e-02   2.05086720e-01\n",
      "   -1.19985579e-01  -5.28578551e-02  -2.12966005e-01  -5.26309664e-01\n",
      "   -2.92950440e-01   6.76294093e-02]\n",
      " [ -9.37080279e-02   2.03404504e-01   1.93714542e-01   6.24757009e-02\n",
      "   -2.47669163e-01   2.01313948e-01   6.33776410e-03  -2.16026378e-02\n",
      "   -4.27729964e-01   3.35986199e-01]\n",
      " [ -6.77272391e-01   1.09585159e-02   4.33551195e-02   1.08380802e-01\n",
      "    2.03304812e-02  -3.30359203e-01   6.28037176e-02  -6.51476262e-01\n",
      "   -6.14822436e-01   6.95301029e-01]\n",
      " [ -5.49956933e-02  -4.09366683e-01  -1.62762206e-01   1.16343850e-01\n",
      "   -5.52585697e-01  -2.12219737e-01  -2.06508317e-01  -1.55512941e-01\n",
      "    3.32195504e-02   8.68500305e-02]\n",
      " [  2.68977857e-01  -6.59876922e-01  -4.12736343e-01   2.17279413e-01\n",
      "    8.34569203e-02   1.38029850e-01   2.02879815e-01  -1.11481184e-01\n",
      "   -7.31147606e-02   5.48813746e-01]\n",
      " [  1.06308352e-01  -4.00047163e-01  -1.68519686e-01   1.34776607e-02\n",
      "   -2.47254371e-01  -2.57391401e-02  -1.30421122e-01   1.42925346e-01\n",
      "    1.35707336e-02   8.24578304e-01]\n",
      " [ -1.05915487e-01  -5.34224836e-01  -6.35199173e-01  -6.15077322e-01\n",
      "   -5.04949011e-01   1.87631297e-01  -4.14219508e-01   3.80145389e-03\n",
      "   -4.72223986e-01   8.84410773e-01]\n",
      " [  6.16518166e-02  -4.36966296e-01  -8.19508797e-02  -6.75035344e-01\n",
      "   -1.96710706e-01  -6.00796062e-01  -6.76004602e-01  -1.85329338e-01\n",
      "   -9.17892650e-02   7.67860739e-01]\n",
      " [ -4.20608531e-01   1.74884661e-01   2.14284261e-01  -5.95488783e-01\n",
      "   -6.83449519e-01  -7.71660620e-02  -2.74606138e-01   1.00794162e-01\n",
      "   -4.03553946e-01   2.67550475e-01]\n",
      " [ -5.48969345e-01  -3.34907358e-01   1.12919597e-01   2.32604823e-01\n",
      "    9.69608782e-02   6.80243322e-02  -5.89184353e-01  -5.18879136e-01\n",
      "   -1.85185500e-01   3.55350358e-01]\n",
      " [ -1.07962173e-01  -3.32084961e-01   1.22065749e-01  -2.67945911e-01\n",
      "   -5.68159154e-01  -4.81129601e-01   1.94812023e-01  -3.82032630e-02\n",
      "   -3.29038525e-01   7.15842253e-01]\n",
      " [ -1.56381611e-01  -5.57632713e-01   1.41927032e-01  -2.28568252e-01\n",
      "   -4.27744179e-01  -3.69351858e-01  -1.27714985e-02  -4.15733378e-01\n",
      "   -2.09282373e-01   8.55661479e-01]\n",
      " [ -6.03786481e-01  -1.04519807e-01  -3.28647414e-01  -1.95060598e-01\n",
      "   -3.61368323e-01   2.74446429e-01  -4.64446033e-02  -4.99141650e-01\n",
      "   -4.88847539e-03   8.52764969e-01]\n",
      " [  3.04913139e-02  -5.20985778e-01   8.75693110e-02  -4.25856319e-01\n",
      "   -3.38003765e-01  -2.73368508e-01   2.12497898e-01  -9.07995135e-02\n",
      "   -4.32446202e-01   7.24638967e-01]\n",
      " [ -4.44907496e-01   5.14022021e-02  -4.73564932e-01  -4.44625665e-01\n",
      "   -6.58022942e-01  -3.06632996e-01  -4.20140537e-01  -1.63952268e-02\n",
      "    2.26911220e-01   8.87137554e-02]\n",
      " [ -1.23096532e-01  -3.21716231e-01  -6.57895046e-01  -3.69051204e-01\n",
      "   -5.14112791e-01  -5.98133536e-01   3.23693655e-02  -9.83618292e-02\n",
      "    1.36586868e-01   4.92709178e-01]\n",
      " [ -3.70730222e-01  -6.86199683e-01  -2.67023676e-01  -2.11206419e-01\n",
      "   -3.35358529e-01  -5.61084321e-02   1.28049211e-01  -5.85424048e-01\n",
      "   -1.02920413e-01   2.77987287e-01]\n",
      " [ -6.29089040e-01  -3.23756713e-01  -4.64517883e-01  -5.23703300e-01\n",
      "   -3.56940737e-01   8.31681324e-02  -1.02876665e-01  -3.98347655e-01\n",
      "   -3.45521289e-01   3.82096920e-01]\n",
      " [ -2.34106797e-02  -5.73229204e-01  -2.11147234e-01   1.19156953e-01\n",
      "   -2.14164234e-01  -6.20959657e-01  -5.02540622e-01  -1.53021517e-01\n",
      "   -3.30081034e-01   4.71091664e-01]\n",
      " [  2.09991525e-01  -4.10876578e-01   6.21297189e-02  -6.81039676e-01\n",
      "   -6.69693348e-01  -1.54615812e-01   2.52763297e-01   1.95318551e-01\n",
      "    2.48208075e-01   7.28457832e-01]\n",
      " [ -1.76620657e-01   1.11467171e-01  -1.33971365e-01  -2.12383550e-01\n",
      "    2.95440186e-02   2.54255206e-01  -5.54174171e-02  -5.23034394e-01\n",
      "    2.74153634e-02   1.15273806e-01]\n",
      " [ -1.96747889e-02  -1.76178609e-01  -4.26515535e-02   1.69325214e-01\n",
      "   -3.14255960e-01  -4.20305302e-01  -3.89126671e-01  -2.66118616e-01\n",
      "    1.59743530e-01   7.12624664e-01]\n",
      " [  2.15409093e-01  -5.82809983e-01  -6.56491092e-02   2.19475832e-01\n",
      "   -5.53314231e-01  -4.16204729e-01   5.92750004e-03  -4.44021286e-01\n",
      "   -5.70253572e-01   5.87455007e-02]\n",
      " [  4.23056241e-02   5.63905609e-02  -6.63215441e-01  -4.59515040e-01\n",
      "    8.09256521e-02   7.35309252e-03   3.45100819e-02   1.89573427e-01\n",
      "    2.64130516e-01   7.33212645e-01]\n",
      " [ -1.64354171e-01  -2.84936931e-01  -5.05253017e-01   1.28963009e-01\n",
      "    1.72280678e-01  -3.53339553e-01  -1.63191647e-01  -3.28223424e-01\n",
      "   -9.20155049e-02   6.73595189e-01]\n",
      " [ -5.97363002e-01   2.71013742e-01   2.58551491e-01  -6.24480149e-01\n",
      "   -6.03261610e-01  -2.02985393e-01   8.95286821e-02  -3.08961023e-03\n",
      "    2.72905579e-01   6.24527350e-01]\n",
      " [  5.50753795e-02  -1.56984717e-01   1.95270144e-01  -6.82220633e-01\n",
      "   -1.70078608e-02  -5.64338530e-01  -4.49942067e-01  -1.77060786e-01\n",
      "    1.98038103e-01  -2.47135894e-02]\n",
      " [  2.77333230e-01  -2.41390949e-01  -4.24786274e-01  -2.04895309e-01\n",
      "   -8.05277238e-02  -4.01475771e-01   2.55304875e-01  -5.13329665e-01\n",
      "   -4.42983322e-01   8.26142110e-02]\n",
      " [ -3.78428297e-01  -3.46972118e-01  -5.48602542e-01  -3.07044715e-02\n",
      "    2.01828292e-01  -3.30951418e-02  -3.71437624e-01  -1.84675957e-01\n",
      "   -5.15434725e-01   8.66623483e-01]\n",
      " [ -6.48794805e-01   1.92107452e-01  -4.51004612e-01   2.49597590e-01\n",
      "   -1.49787222e-01  -6.57883909e-01  -6.14820519e-01   1.70863604e-01\n",
      "    1.34369981e-01   6.99003732e-01]\n",
      " [  8.74889831e-02  -3.10372332e-01  -4.77734599e-01  -6.66282169e-01\n",
      "   -2.72637934e-01  -2.33649681e-01   9.13269622e-02  -6.71883568e-01\n",
      "    1.41729119e-01   9.31535972e-01]\n",
      " [ -1.04378045e-01  -5.57186249e-01  -3.43315803e-01  -3.31081106e-02\n",
      "   -1.56430342e-01  -6.37969245e-01  -1.04796826e-02   1.74843745e-01\n",
      "   -4.68798857e-01   8.39799925e-01]\n",
      " [  1.11882905e-01   1.59739238e-02  -2.25275121e-01  -5.99661694e-03\n",
      "    2.51013364e-01   5.86313114e-02   1.63262316e-01   1.84665166e-01\n",
      "   -3.43224394e-01  -3.50922151e-02]\n",
      " [ -2.94830235e-02  -1.12625268e-01  -5.28197110e-01  -3.61350920e-01\n",
      "   -5.03770412e-01  -5.18013436e-01  -4.20545672e-01  -8.00567572e-02\n",
      "    6.63018113e-02   2.46978874e-01]\n",
      " [ -2.32661488e-01   2.51367249e-01   8.61210040e-02  -1.26411918e-01\n",
      "   -3.59422286e-01  -1.35658198e-01   1.50670501e-01  -4.77498940e-01\n",
      "   -5.15803954e-01   4.99440548e-01]\n",
      " [  1.50261142e-01   2.62383157e-01  -2.25327603e-01   2.94166639e-01\n",
      "   -4.02304195e-01  -6.40766083e-01  -4.71170918e-01  -6.83126224e-01\n",
      "   -6.87966217e-01   4.47386945e-01]\n",
      " [ -6.75800677e-01  -1.29384066e-02  -4.09498434e-01   2.03383999e-01\n",
      "   -2.41659284e-01  -1.92061765e-01  -5.26961666e-01  -4.21716926e-01\n",
      "   -2.19859127e-01   8.64196860e-01]\n",
      " [ -4.75120614e-01   8.86953946e-02  -2.05219078e-01  -5.13010244e-01\n",
      "   -5.59299846e-01   2.23418041e-02  -4.68530172e-01  -8.36039879e-02\n",
      "   -5.32282768e-01   9.35196983e-01]\n",
      " [ -6.56994693e-01  -3.99911628e-01   2.45114210e-01  -3.56297084e-01\n",
      "   -5.62596285e-01  -6.10347202e-01  -5.97237762e-01  -3.22916331e-01\n",
      "    2.53401537e-01   2.00439035e-01]\n",
      " [ -5.40447175e-01  -6.85337203e-01   9.63042774e-03  -4.41500195e-01\n",
      "   -5.51158101e-01  -1.89486150e-02   3.19002756e-02  -2.43446962e-01\n",
      "   -8.49108037e-02   8.64189822e-02]\n",
      " [ -5.48650823e-01   1.77404793e-01   7.71430373e-02  -6.38712098e-01\n",
      "   -1.32541861e-01   2.90896361e-01   9.67244854e-04  -4.00759020e-01\n",
      "   -6.27582842e-02   6.79890341e-01]\n",
      " [  2.24120678e-01  -2.05103383e-01   2.18610100e-01   1.07638054e-01\n",
      "   -3.27472976e-01  -3.26136768e-01  -8.27115179e-02  -3.57807042e-01\n",
      "    1.57562837e-01   8.20441290e-01]\n",
      " [ -4.55954693e-01  -1.27861538e-01  -4.74307100e-01  -2.08167826e-01\n",
      "   -6.23119269e-01  -7.23108625e-02  -3.40556002e-01  -1.31596872e-01\n",
      "   -2.86652725e-01   1.62908827e-01]\n",
      " [ -2.97435014e-01   5.26608580e-02  -3.51943562e-01  -3.29697700e-01\n",
      "    1.78982723e-01  -5.17322865e-01  -3.97428336e-01  -3.68336401e-01\n",
      "   -3.07888470e-02  -4.15322339e-03]\n",
      " [ -6.49301193e-01   1.14090898e-02   1.41584080e-01  -9.30655141e-03\n",
      "    3.62718356e-03  -2.97474528e-01  -5.62127024e-01  -6.66352337e-01\n",
      "   -1.20736862e-01   6.50966848e-03]\n",
      " [ -6.65464526e-01   1.82962583e-01  -3.15321980e-01   2.18854762e-01\n",
      "   -2.05922798e-01  -3.52794850e-01  -5.25345258e-02  -6.56804792e-01\n",
      "    1.72427855e-01   3.24582064e-01]\n",
      " [ -2.34606540e-01  -8.89984237e-03  -4.17127864e-01  -6.00891398e-02\n",
      "   -2.42061000e-02   2.12921884e-01  -4.66148814e-01  -1.91230527e-02\n",
      "    2.21770696e-01   7.15396957e-01]\n",
      " [  2.90575086e-01  -4.42754483e-01  -1.27971803e-01   2.30253180e-01\n",
      "   -1.47862142e-01  -5.01036394e-01  -6.39745353e-01   9.31245539e-02\n",
      "   -3.30478166e-01   8.97991707e-01]\n",
      " [ -2.41451406e-01  -4.71019487e-01  -4.82262242e-01   1.36172540e-01\n",
      "   -5.86793483e-01   2.82334226e-01  -3.35712949e-01   2.18359584e-01\n",
      "   -5.99640603e-01   8.11215433e-01]\n",
      " [  8.00350471e-02   1.95895363e-01  -6.31230626e-01  -4.13922383e-01\n",
      "   -2.94936817e-01   2.15136521e-01   2.44176167e-01  -4.48959866e-01\n",
      "   -5.44087019e-01   1.03921835e-01]\n",
      " [ -1.10275875e-01  -2.09267575e-01   1.62292457e-01   2.70287334e-01\n",
      "   -5.87094202e-01  -1.89836986e-01  -2.23472204e-01  -4.36822697e-02\n",
      "   -1.68568572e-01   1.21857806e-01]\n",
      " [ -1.67389824e-01  -3.64162246e-01  -6.57197995e-01   2.32163161e-01\n",
      "    2.10898703e-01  -1.87393533e-01  -2.75212653e-01  -3.23926852e-01\n",
      "   -6.57031987e-01   2.63202067e-01]\n",
      " [ -2.86100818e-01  -5.64893801e-01   5.70389182e-02  -3.86696467e-01\n",
      "    1.91042998e-01  -6.73273655e-01  -6.25840453e-01   1.68975159e-01\n",
      "   -1.06418241e-02   5.95973350e-01]\n",
      " [ -2.34969181e-01  -1.94200918e-01  -1.54322414e-01   1.13634349e-02\n",
      "   -9.06473456e-02   6.35591521e-02  -2.88021500e-01   2.82327797e-01\n",
      "   -2.96839863e-01   7.65691717e-01]\n",
      " [  1.20355232e-01  -5.04040703e-01   2.44613474e-01  -5.87946927e-02\n",
      "   -4.46949932e-01   1.69662215e-01  -3.58450884e-01   2.68620701e-01\n",
      "    1.45485820e-02   8.82660032e-01]\n",
      " [  5.64670128e-02  -1.23367379e-01  -5.97273668e-01  -9.89340239e-02\n",
      "    8.97228551e-02   5.06711328e-02  -1.51550484e-01  -5.30435050e-01\n",
      "   -5.33795495e-01   8.96681298e-01]\n",
      " [ -4.88409266e-02  -3.51740008e-01  -1.37367756e-01  -2.31311044e-01\n",
      "   -2.40350126e-01   1.82405332e-01  -6.73495848e-02  -6.45299487e-01\n",
      "    1.44182006e-01   7.83113958e-01]\n",
      " [ -3.81173903e-01   1.80850147e-01  -6.50056118e-01  -3.98756838e-01\n",
      "   -2.77826920e-02   2.21925581e-01   3.01267059e-01  -2.56416343e-01\n",
      "    2.57961378e-01   1.44788647e-02]\n",
      " [  1.92880001e-01   5.53389885e-02  -9.05997589e-02   7.02132209e-02\n",
      "   -6.47495946e-01  -5.49095583e-01   1.27433743e-01  -1.21812687e-01\n",
      "   -9.63115475e-02   6.12796962e-01]\n",
      " [ -3.98268311e-01   9.53173488e-02  -6.31123293e-01  -1.99593205e-01\n",
      "    6.01188364e-02  -4.28584480e-01  -4.78301581e-01  -1.22306285e-01\n",
      "   -2.09678107e-01   6.23496854e-01]\n",
      " [ -5.52059716e-01  -2.09901809e-01  -6.03782608e-01  -1.46942538e-01\n",
      "   -6.67174380e-01  -4.57241034e-01  -1.27816442e-01   2.37616678e-01\n",
      "   -1.49682314e-01   7.84174753e-01]\n",
      " [ -2.97442389e-01   7.96296249e-02  -5.56778469e-01  -1.79656185e-01\n",
      "   -3.57027549e-02   1.19299367e-01  -3.89539756e-01  -5.49860282e-01\n",
      "   -1.29689695e-01   9.18456099e-02]\n",
      " [  1.30703092e-01   2.63854029e-01  -6.51529623e-01   1.21153087e-01\n",
      "    6.03670864e-02  -6.66799573e-01   1.96422237e-02  -6.12620784e-01\n",
      "   -5.29904358e-01   5.93212909e-01]\n",
      " [ -5.80768110e-01  -2.68817114e-01  -5.01793385e-01  -3.27501961e-01\n",
      "   -1.10642449e-01  -9.78939572e-02  -4.05917798e-02  -5.78537570e-01\n",
      "    5.20062176e-02   7.71932994e-01]\n",
      " [ -3.31207485e-01  -6.17329590e-01   2.87061819e-01  -6.79576527e-01\n",
      "   -6.58921413e-01  -6.22731687e-01   1.34238683e-01  -4.13506113e-02\n",
      "   -2.07481750e-01   1.67039768e-01]\n",
      " [ -2.84138960e-03   3.00029023e-02   5.70326304e-02   2.10309900e-01\n",
      "    1.24227669e-01   2.60330371e-01   1.88575476e-02   1.48319811e-01\n",
      "    9.79109101e-02   6.81604996e-01]\n",
      " [ -3.11793039e-01  -6.37816475e-01  -1.20191499e-01   3.00874932e-01\n",
      "   -4.61640780e-01  -3.53289605e-02  -4.30337904e-02  -2.92655346e-01\n",
      "   -5.42716169e-01   7.13166041e-01]\n",
      " [ -6.35374962e-01  -1.66830456e-01  -5.23651207e-01  -4.42028066e-01\n",
      "    4.57571816e-02  -6.23182838e-01  -9.34912532e-02   9.18767589e-02\n",
      "   -5.20196692e-02   1.37513788e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/test/python_test/cs231n/assignment3/py3/lib/python3.5/site-packages/ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "/home/wei/test/python_test/cs231n/assignment3/py3/lib/python3.5/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/wei/test/python_test/cs231n/assignment3/py3/lib/python3.5/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in log\n",
      "/home/wei/test/python_test/cs231n/assignment3/py3/lib/python3.5/site-packages/ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in log\n",
      "/home/wei/test/python_test/cs231n/assignment3/py3/lib/python3.5/site-packages/ipykernel_launcher.py:70: RuntimeWarning: divide by zero encountered in log\n",
      "/home/wei/test/python_test/cs231n/assignment3/py3/lib/python3.5/site-packages/ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "#PROBLEM 6\n",
    "def update_weights_double_layer_batch_act(X, Y, weights, bias, lr, batch_size, activation):\n",
    "    #INSERT YOUR CODE HERE\n",
    "    if activation == 'sigmoid':\n",
    "        #INSERT YOUR CODE HERE\n",
    "        activate_function = lambda x : 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "    if activation == 'tanh':\n",
    "        #INSERT YOUR CODE HERE\n",
    "        activate_function = lambda x : np.tanh(x)\n",
    "    if activation == 'relu':\n",
    "        #INSERT YOUR CODE HERE\n",
    "        activate_function = lambda x : np.maximum(0,x)\n",
    "    #INSERT YOUR CODE HERE\n",
    "    cache = {'a':[],'z':[]}\n",
    "    num_batch = Y.shape[0] // batch_size\n",
    "    Y_hat = np.zeros([Y.shape[0],10])\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y_hat[i][Y[i]] = 1\n",
    "    for i_batch in range(num_batch):\n",
    "        cache['a'].clear()\n",
    "        cache['z'].clear()\n",
    "        z = X[i_batch*batch_size:(i_batch+1)*batch_size,:]\n",
    "        cache['z'].append(z)\n",
    "        for i in range(len(weights)-1):\n",
    "            a = np.matmul(z, weights[i]) + bias[i]\n",
    "            z = activate_function(a)\n",
    "            cache['a'].append(a)\n",
    "            cache['z'].append(z)\n",
    "        a = np.matmul(z, weights[len(weights)-1]) + bias[len(weights)-1]\n",
    "#         output = np.exp(a) / (1.0 + np.exp(a))\n",
    "        output = activate_function(a)\n",
    "        cache['a'].append(a)\n",
    "        cache['z'].append(output)\n",
    "        loss = -1.0 * (Y_hat[i_batch*batch_size:(i_batch+1)*batch_size,:]*np.log(output) + (1.0 - Y_hat[i_batch*batch_size:(i_batch+1)*batch_size,:])*np.log(1.0 - output))\n",
    "        #backpropagation\n",
    "        da = output - Y_hat[i_batch*batch_size:(i_batch+1)*batch_size,:]\n",
    "        dw = np.dot(cache['z'][-2].T, da)\n",
    "        db = np.sum(da,axis = 0)\n",
    "        weights[-1] += -1.0*lr*dw\n",
    "        bias[-1]    += -1.0*lr*db\n",
    "        for i in range(len(weights)-1,0,-1):\n",
    "            dz = np.dot(da,weights[i].T)\n",
    "            print(cache['z'][i].shape)\n",
    "            print(dz.shape)\n",
    "            da = dz*(1.0 - cache['z'][i])*cache['z'][i]\n",
    "            dw = np.dot(cache['z'][i-1].T,da)\n",
    "            db = np.sum(da,axis=0)\n",
    "            weights[i-1] += -1.0*lr*dw\n",
    "            bias[i-1]    += -1.0*lr*db\n",
    "        print(i_batch)\n",
    "            \n",
    "#     last batch\n",
    "    if(num_batch*batch_size < Y.shape[0]):\n",
    "        cache['a'].clear()\n",
    "        cache['z'].clear()\n",
    "        z = X[num_batch*batch_size:,:]\n",
    "        print(z.shape)\n",
    "        cache['z'].append(z)\n",
    "        for i in range(len(weights)-1):\n",
    "            a = np.matmul(z, weights[i]) + bias[i]\n",
    "            z = activate_function(a)\n",
    "            cache['a'].append(a)\n",
    "            cache['z'].append(z)\n",
    "        a = np.matmul(z, weights[len(weights)-1]) + bias[len(weights)-1]\n",
    "        print(z.shape)\n",
    "#         output = np.exp(a) / (1.0 + np.exp(a))\n",
    "        output = activate_function(a)\n",
    "        cache['a'].append(a)\n",
    "        cache['z'].append(output)\n",
    "        loss = -1.0 * (Y_hat[num_batch*batch_size:,:]*np.log(output) + (1.0 - Y_hat[num_batch*batch_size:,:])*np.log(1.0 - output))\n",
    "        #backpropagation\n",
    "        da = output - Y_hat[num_batch*batch_size:,:]\n",
    "        dw = np.dot(cache['z'][-2].T, da)\n",
    "        db = np.sum(da,axis = 0)\n",
    "        weights[-1] += -1.0*lr*dw\n",
    "        bias[-1]    += -1.0*lr*db\n",
    "        for i in range(len(weights)-1,0,-1):\n",
    "            dz = np.dot(da,weights[i].T)\n",
    "            print(cache['z'][i].shape)\n",
    "            print(da.shape)\n",
    "            da = dz*(1.0 - cache['z'][i])*cache['z'][i]\n",
    "            dw = np.dot(cache['z'][i-1].T,da)\n",
    "            db = np.sum(da,axis=0)\n",
    "            weights[i-1] += -1.0*lr*dw\n",
    "            bias[i-1]    += -1.0*lr*db\n",
    "        print(num_batch)\n",
    "    updated_weights, updated_bias = weights, bias\n",
    "    return updated_weights, updated_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM 7\n",
    "def update_weights_double_layer_batch_act_mom(X, Y, weights, bias, lr, batch_size, activation, momentum):\n",
    "    #INSERT YOUR CODE HERE\n",
    "    if activation == 'sigmoid':\n",
    "        #INSERT YOUR CODE HERE\n",
    "        activate_function = lambda x : 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "    if activation == 'tanh':\n",
    "        #INSERT YOUR CODE HERE\n",
    "        activate_function = lambda x : np.tanh(x)\n",
    "    if activation == 'relu':\n",
    "        #INSERT YOUR CODE HERE\n",
    "        activate_function = lambda x : np.maximum(0,x)\n",
    "    #INSERT YOUR CODE HERE\n",
    "    return updated_weights, updated_bias\n",
    "    \n",
    "def main():\n",
    "    X, Y = load_data(\"./digitstrain.txt\")\n",
    "    weights = []\n",
    "    bias = []\n",
    "    batch_size = 333\n",
    "    weights.append(np.random.rand(784,100))\n",
    "    bias.append(np.random.rand(100))\n",
    "    weights.append(np.random.rand(100,100))\n",
    "    bias.append(np.random.rand(100))\n",
    "    weights.append(np.random.rand(100,10))\n",
    "    bias.append(np.random.rand(10))\n",
    "    learning_rate = 1e-3\n",
    "    activation = 'tanh'\n",
    "#     weights, bias = update_weights_perceptron(X,Y,weights,bias,learning_rate)\n",
    "#    weights, bias = update_weights_double_layer(X, Y, weights, bias, learning_rate)\n",
    "#     weights,bias = update_weights_double_layer_batch(X, Y, weights, bias, learning_rate, batch_size)\n",
    "    weights, bias = update_weights_double_layer_batch_act(X, Y, weights, bias, learning_rate, batch_size, activation)\n",
    "    print(weights[-1])\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
